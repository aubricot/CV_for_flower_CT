{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/CV_for_flower_CT/blob/main/Cacao_Whole_Flower_Seg_unetr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iHpIGLEPFOB"
      },
      "source": [
        "# Using MONAI Label for shape prediction of flowers from micro-CT images\n",
        "---   \n",
        "*Last Updated 15 Mar 2023*  \n",
        "-Runs in Python 3 with Monai 1.5-   \n",
        "This notebook uses a dataset of micro-CT images and segementations (both as nifti files, .nii.gz) to custom train a UNETR Computer Vision Model ([Hatamizadeh et al. 2021](https://arxiv.org/abs/2103.10504)) using the [MONAI Label](https://github.com/aubricot/computer_vision_with_eol_images/tree/master/classification_for_image_tagging) framework. U-Nets are a type of Convolutional Neural Network (CNN) shaped like a \"U\" with pairings of encoders (contracting path) and decoders (expanding path) who have gained popularity in recent years as a tool for segmentation of 3D image datasets ([Tomar 2021](https://medium.com/analytics-vidhya/what-is-unet-157314c87634&ved=2ahUKEwjeiK-myoyMAxX-QzABHb85DmQQFnoECBEQAQ&usg=AOvVaw1V_3fmv2Fckxy0QTuwS6LM)). UNETR has a similar network design/shape for the encoder and decoder as a U-Net, but UNEt TRansformers (UNETR), uses a transformer as the encoder.\n",
        "\n",
        "Notes\n",
        "* dd\n",
        "\n",
        "_References_\n",
        "\n",
        "* [Hatamizadeh et al. 2021](https://arxiv.org/abs/2103.10504)   \n",
        "* [Tomar 2021](https://medium.com/analytics-vidhya/what-is-unet-157314c87634&ved=2ahUKEwjeiK-myoyMAxX-QzABHb85DmQQFnoECBEQAQ&usg=AOvVaw1V_3fmv2Fckxy0QTuwS6LM)\n",
        "\n",
        "_License_   \n",
        "This notebook is licensed under an [MIT license](https://github.com/aubricot/CV_for_flower_CT/blob/master/LICENSE). Portions are modified from the MONAI Label [3D Multi-organ Segmentation with UNETR (BTCV Challenge)](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/unetr_btcv_segmentation_3d.ipynb) following licensing terms under Copyright (c) MONAI Consortium and [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).\n",
        "\n",
        "The sections **X, Y, Z** were modified to better suit our needs. Google Colab form fields were also added, as well as tools to split the dataset train-validate-test, methods to save training outputs (trained models, notebook in its state for each train run as a txt file, and train graphs; named based on the train attempt number (ie the first train attempt files were saved as 01.ph, 01.txt, and 01.png)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs & Imports\n",
        "---"
      ],
      "metadata": {
        "id": "ZG6BekRX0YUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make sure runtime is set to GPU and High RAM\n",
        "# Copyright (c) MONAI Consortium and [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "IxiZicF74BXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check how many workers are suggested for your system\n",
        "# TO DO: Adjust num_workers throughout code to speed or slow performance based on available (cloud) hardware\n",
        "import os\n",
        "\n",
        "cpu_count = os.cpu_count()\n",
        "if cpu_count is not None:\n",
        "    max_num_worker_suggest = cpu_count\n",
        "\n",
        "print(\"Suggested max number of workers is: \", max_num_worker_suggest)"
      ],
      "metadata": {
        "id": "Z45TOUuL2we9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose where to save results\n",
        "\n",
        "# Use dropdown menu on right\n",
        "save = \"in my Google Drive\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "\n",
        "# Mount google drive to export image tagging file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Type in the path to your project wd in form field on right\n",
        "wd = \"/content/drive/MyDrive/Miami_21/project/manuscripts/CV_for_flower_CT/analysis/data\" # @param [\"/content/drive/MyDrive/Miami_21/project/manuscripts/CV_for_flower_CT/analysis/data\",\"/content/drive/MyDrive/Miami_21/project/manuscripts/CV_for_flower_CT/analysis/demo_data/Abdomen/Abdomen/RawData/data\"] {\"allow-input\":true}\n",
        "print(\"Working with data from: \\n\")\n",
        "%cd $wd"
      ],
      "metadata": {
        "id": "RzKDsTXQfAmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDXTgDy0PFOE"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "!pip install -q \"monai-weekly[nibabel, tqdm, einops]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!pip install nbconvert\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-myucD_YPFOG"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import glob\n",
        "import math\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from monai.utils import first\n",
        "from monai.losses import DiceCELoss, DiceLoss\n",
        "from monai.inferers import sliding_window_inference, SimpleInferer\n",
        "from monai.transforms import (\n",
        "    Activations,\n",
        "    AsDiscrete,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    EnsureChannelFirstd,\n",
        "    EnsureType,\n",
        "    EnsureTyped,\n",
        "    FgBgToIndicesd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandFlipd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    RandShiftIntensityd,\n",
        "    RandSpatialCropd,\n",
        "    Resized,\n",
        "    ScaleIntensityRangePercentilesd,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    RandRotate90d,\n",
        ")\n",
        "\n",
        "from monai.config import print_config\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.networks.layers import Act, Norm\n",
        "from monai.networks.nets import UNETR, unet\n",
        "from torch.optim import Adam, SGD\n",
        "from monai.optimizers import Novograd\n",
        "from monai.apps import download_and_extract\n",
        "from monai.data import NibabelReader\n",
        "\n",
        "from monai.data import (\n",
        "    DataLoader,\n",
        "    CacheDataset,\n",
        "    load_decathlon_datalist,\n",
        "    decollate_batch,\n",
        "    ThreadDataLoader,\n",
        "    Dataset,\n",
        "    set_track_meta,\n",
        ")\n",
        "\n",
        "# Facilitate debugging by making GPU tell more useful information\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "\n",
        "# Check that cuda is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available and being used.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available, using CPU instead.\")\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set up data directory\n",
        "os.environ['MONAI_DATA_DIRECTORY'] = wd\n",
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = directory\n",
        "print(f\"root dir is: {root_dir}\")\n",
        "\n",
        "out_dir = wd + '/' + \"results/\"\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "print(f\"out dir is: {out_dir}\")"
      ],
      "metadata": {
        "id": "XOMgBexnmrIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define functions\n",
        "\n",
        "# Save each new training attempt results by name (TRAIN_SES_NUM)\n",
        "def set_saved_model_path(out_dir = out_dir, saved_models_dir = \"saved_models\"):\n",
        "    saved_models_wd = os.path.join(out_dir, saved_models_dir)\n",
        "    if not os.path.exists(saved_models_wd):\n",
        "        print(\"{} not found. Making saved models directory at: {}\".format(saved_models_dir, saved_models_wd))\n",
        "        os.mkdir(saved_models_wd)\n",
        "    # Name folder to sort by attempt number (useful if many training runs)\n",
        "    else:\n",
        "        try:\n",
        "            last_attempt = !ls $saved_models_wd | tail -n 1\n",
        "            last_attempt = int(os.path.splitext(last_attempt[0])[0])\n",
        "        except:\n",
        "            last_attempt = 0\n",
        "        if last_attempt < 9:\n",
        "            TRAIN_SESS_NUM = \"0\" + str(last_attempt + 1)\n",
        "        else:\n",
        "            TRAIN_SESS_NUM = str(last_attempt + 1)\n",
        "\n",
        "    saved_model_path = os.path.join(saved_models_wd, (TRAIN_SESS_NUM + \".pth\"))\n",
        "    print(\"Saving trained model results to: \", saved_model_path)\n",
        "\n",
        "    return saved_model_path, TRAIN_SESS_NUM\n",
        "\n",
        "# For pulling img and label file names from train/val dictionaries\n",
        "def find_dict_idx_by_val(dict_list, key, val):\n",
        "    for idx, dict in enumerate(dict_list):\n",
        "        if key in dict and val in dict[key]:\n",
        "            return idx"
      ],
      "metadata": {
        "id": "I96sR9xO3TZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1bZDzqoPFOI"
      },
      "source": [
        "## Setup transforms for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train-test-validation split\n",
        "!pip install split-folders\n",
        "import splitfolders\n",
        "\n",
        "# List all files in dir before splitting\n",
        "all_vols = os.listdir('./images/class1')\n",
        "n = len(all_vols)\n",
        "print(\"Training dataset size of N = \", n)\n",
        "\n",
        "# Save list of files before split to txt file in your wd\n",
        "with open(\"all_vols.txt\", \"w\") as output:\n",
        "    output.write(str(all_vols))\n",
        "print(\"List of all files before splitting saved to: \", (wd + '/all_vols.txt'))\n",
        "\n",
        "# Train-test-validation split (70% - 15% - 15%)\n",
        "splitfolders.ratio(\"images\", output=\"input\", seed=n,\n",
        "                   ratio=(0.7, 0.15, 0.15), group_prefix=None, move=False)"
      ],
      "metadata": {
        "id": "GH8jTIcizTyR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Move labels to their corresponding train-test-val folders to match images\n",
        "\n",
        "# Get list of all labels\n",
        "all_labs = os.listdir('./labels/class1')\n",
        "all_labs = [os.path.splitext(os.path.splitext(file)[0])[0] for file in all_labs] # Remove .nii.gz extensions\n",
        "all_labs = [file.removesuffix('_seg') for file in all_labs]\n",
        "\n",
        "# Find which label files need to be moved to train-test-val and move them\n",
        "train = os.listdir('./input/train/class1')\n",
        "train = [os.path.splitext(os.path.splitext(file)[0])[0] for file in train] # Remove .nii.gz extensions\n",
        "train = [file.removesuffix('_crop') for file in train]\n",
        "train = [file.removesuffix('_cropped') for file in train]\n",
        "train = [file.removesuffix('_crop_mask') for file in train]\n",
        "\n",
        "test = os.listdir('./input/test/class1')\n",
        "test = [os.path.splitext(os.path.splitext(file)[0])[0] for file in test] # Remove .nii.gz extensions\n",
        "test = [file.removesuffix('_crop') for file in test]\n",
        "test = [file.removesuffix('_cropped') for file in test]\n",
        "test = [file.removesuffix('_crop_mask') for file in test]\n",
        "\n",
        "val = os.listdir('./input/val/class1')\n",
        "val = [os.path.splitext(os.path.splitext(file)[0])[0] for file in val] # Remove .nii.gz extensions\n",
        "val = [file.removesuffix('_crop') for file in val]\n",
        "val = [file.removesuffix('_cropped') for file in val]\n",
        "val = [file.removesuffix('_crop_mask') for file in val]\n",
        "\n",
        "train_mv = set(train) & set(all_labs)\n",
        "test_mv = set(test) & set(all_labs)\n",
        "val_mv = set(val) & set(all_labs)\n",
        "\n",
        "# Move files to train\n",
        "os.makedirs('input/train/class1/labels')\n",
        "for file in train_mv:\n",
        "    file_path = os.path.join('labels/class1', file + '_seg.nii.gz')\n",
        "    target = os.path.join('input/train/class1/labels', file + '_seg.nii.gz')\n",
        "    shutil.copy2(file_path, target)  # Preserves metadata\n",
        "    print(f\"Copied: {file_path} to {target}\")\n",
        "\n",
        "# Move files to test\n",
        "os.makedirs('input/test/class1/labels')\n",
        "for file in test_mv:\n",
        "    file_path = os.path.join('labels/class1', file + '_seg.nii.gz')\n",
        "    target = os.path.join('input/test/class1/labels', file + '_seg.nii.gz')\n",
        "    shutil.copy2(file_path, target)  # Preserves metadata\n",
        "    print(f\"Copied: {file_path} to {target}\")\n",
        "\n",
        "# Move files to validation (val)\n",
        "os.makedirs('input/val/class1/labels')\n",
        "for file in val_mv:\n",
        "    file_path = os.path.join('labels/class1', file + '_seg.nii.gz')\n",
        "    target = os.path.join('input/val/class1/labels', file + '_seg.nii.gz')\n",
        "    shutil.copy2(file_path, target)  # Preserves metadata\n",
        "    print(f\"Copied: {file_path} to {target}\")\n",
        "\n",
        "# Move all image files within train-test-val into images/\n",
        "target = 'input/train/class1/images'\n",
        "os.makedirs(target)\n",
        "source = './input/train/class1'\n",
        "files = os.listdir(source)\n",
        "for file in files:\n",
        "    shutil.move(os.path.join(source, file), target)\n",
        "    print(f\"Moved: {source} to {target}\")\n",
        "\n",
        "target = 'input/test/class1/images'\n",
        "os.makedirs(target)\n",
        "source = './input/test/class1'\n",
        "files = os.listdir(source)\n",
        "for file in files:\n",
        "    shutil.move(os.path.join(source, file), target)\n",
        "    print(f\"Moved: {source} to {target}\")\n",
        "\n",
        "target = 'input/val/class1/images'\n",
        "os.makedirs(target)\n",
        "source = './input/val/class1'\n",
        "files = os.listdir(source)\n",
        "for file in files:\n",
        "    shutil.move(os.path.join(source, file), target)\n",
        "    print(f\"Moved: {source} to {target}\")"
      ],
      "metadata": {
        "id": "Pqu-8yuhAhMw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform dataset"
      ],
      "metadata": {
        "id": "Z35Y-MlcPjcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get dataset filepaths for train and validation\n",
        "data_root = os.path.join(root_dir, \"input\")\n",
        "\n",
        "train_images = sorted(glob.glob(os.path.join(data_root, \"train/images\", \"*.nii.gz\")))\n",
        "train_labels = sorted(glob.glob(os.path.join(data_root, \"train/labels\", \"*.nii.gz\")))\n",
        "val_images = sorted(glob.glob(os.path.join(data_root, \"val/images\", \"*.nii.gz\")))\n",
        "val_labels = sorted(glob.glob(os.path.join(data_root, \"val/labels\", \"*.nii.gz\")))\n",
        "\n",
        "train_files = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
        "val_files = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(val_images, val_labels)]\n",
        "num_train = len(train_files)\n",
        "num_val = len(val_files)\n",
        "\n",
        "print(\"Training model with {} train images and {} validation images\\n\".format(num_train, num_val))\n",
        "print(\" Train images loaded from {} \\n Train labels loaded from {} \\n Validation images loaded from {} \\n Validation labels loaded from {}\".format(train_images, train_labels, val_images, val_labels))"
      ],
      "metadata": {
        "id": "kzKIGvLJrjV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Transforms\n",
        "\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(.05, .05, .05),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRangePercentilesd(\n",
        "                keys=\"image\",\n",
        "                lower=10, upper=90, b_min=0, b_max=200,\n",
        "                clip=False,\n",
        "                relative=False,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        RandCropByPosNegLabeld(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            label_key=\"label\",\n",
        "            spatial_size=(96, 96, 96),\n",
        "            pos=1,\n",
        "            neg=1,\n",
        "            num_samples=4,\n",
        "            image_key=\"image\",\n",
        "            image_threshold=0,\n",
        "        ),\n",
        "        RandFlipd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            spatial_axis=[0],\n",
        "            prob=0.10,\n",
        "        ),\n",
        "        RandFlipd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            spatial_axis=[1],\n",
        "            prob=0.10,\n",
        "        ),\n",
        "        RandFlipd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            spatial_axis=[2],\n",
        "            prob=0.10,\n",
        "        ),\n",
        "        RandRotate90d(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            prob=0.10,\n",
        "            max_k=3,\n",
        "        ),\n",
        "        RandShiftIntensityd(\n",
        "            keys=[\"image\"],\n",
        "            offsets=0.10,\n",
        "            prob=0.50,\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(.05, .05, .05),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRangePercentilesd(\n",
        "                keys=\"image\",\n",
        "                lower=10, upper=90, b_min=0, b_max=200,\n",
        "                clip=False,\n",
        "                relative=False,),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        #RandCropByPosNegLabeld(\n",
        "            #keys=[\"image\", \"label\"],\n",
        "            #label_key=\"label\",\n",
        "            #spatial_size=(96, 96, 96),\n",
        "            #pos=1,\n",
        "            #neg=1,\n",
        "            #num_samples=4,\n",
        "            #image_key=\"image\",\n",
        "            #image_threshold=0,\n",
        "       # ),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "9XTwTQrKHIJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Transform datasets and read into cache for training\n",
        "\n",
        "# Train Dataset\n",
        "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_num=num_train, cache_rate=1.0, num_workers=8)\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Validation Dataset\n",
        "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_num=num_val, cache_rate=1.0, num_workers=8)\n",
        "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"\\n\\nSample Validation image shape after transform: {}, label shape: {}\".format(first(val_loader)['image'].shape, first(val_loader)['label'].shape))\n",
        "print(\"\\n\\nSample Train image shape after transform: {}, label shape: {}\".format(first(train_loader)['image'].shape, first(train_loader)['label'].shape))"
      ],
      "metadata": {
        "id": "yESVL-6zIOLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set index for image to check\n",
        "## Katie to do: Still in progress\n",
        "import nibabel as nib\n",
        "\n",
        "# Coordinate system (for Nifti it's RAS)\n",
        "axcodes = \"RAS\" # @param [\"RAS\"] {\"allow-input\":true}\n",
        "\n",
        "# Check transformed image shape and loading\n",
        "load_transform = Compose(\n",
        "    [\n",
        "        LoadImaged(reader = NibabelReader(), keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"], strict_check=False, channel_dim=-1),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=axcodes),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(.05, .05, .05),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "img_idx = 4 # @param {\"type\":\"slider\",\"min\":0,\"max\":5,\"step\":1}\n",
        "slice_num = 186 # @param {\"type\":\"slider\",\"min\":0,\"max\":500,\"step\":1}\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Check an image from the val dataset\n",
        "    img_name = os.path.split(val_ds[img_idx]['image'].meta[\"filename_or_obj\"])[1]\n",
        "    print(\"Visualizing image transforms for: {} ...\\n\".format(img_name))\n",
        "\n",
        "    # Get original image and label info\n",
        "    img_path = data_root + '/val/images/' + img_name\n",
        "    # Workaround for _crop in image filenames not in labels\n",
        "    if '_crop' in img_name:\n",
        "        img_name = str.split(img_name, '_crop')[0]\n",
        "    idx = find_dict_idx_by_val(val_files, 'label', img_name)\n",
        "    orig_data = load_transform(val_files[idx])\n",
        "    orig_img_data = orig_data[\"image\"]\n",
        "    orig_img = torch.unsqueeze(orig_img_data, 0)\n",
        "    print(\"Original image shape: \", orig_img.shape)\n",
        "    orig_label_data = orig_data[\"label\"]\n",
        "    orig_label = torch.unsqueeze(orig_label_data, 0)\n",
        "    print(\"Original label shape: \", orig_label.shape)\n",
        "\n",
        "    # Get transformed image and label info\n",
        "    img = val_ds[img_idx][\"image\"]\n",
        "    label = val_ds[img_idx][\"label\"]\n",
        "    print(\"Transformed image shape: \", img.shape)\n",
        "    print(\"Transformed label shape: \", label.shape)\n",
        "\n",
        "    # Plot images\n",
        "    plt.figure(\"Orig vs Transformed\", (22, 6))\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.title(\"original image\")\n",
        "    plt.imshow(orig_img.numpy()[0, :, :, slice_num], cmap='gray')\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.title(\"original label\")\n",
        "    plt.imshow(orig_label.numpy()[0, :, :, slice_num], cmap='gray')\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.title(\"transformed image\")\n",
        "    plt.imshow(img.cpu().numpy()[0, :, :, slice_num], cmap=\"gray\")\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.title(\"transformed label\")\n",
        "    plt.imshow(label.cpu().numpy()[0, :, :, slice_num])\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kLEKmh_fL_yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set index for image to check\n",
        "## Katie to do: Still in progress\n",
        "import nibabel as nib\n",
        "\n",
        "img_idx = 4 # @param {\"type\":\"slider\",\"min\":0,\"max\":5,\"step\":1}\n",
        "slice_num = 186 # @param {\"type\":\"slider\",\"min\":0,\"max\":500,\"step\":1}\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Check an image from the val dataset\n",
        "    img_name = os.path.split(val_ds[img_idx]['image'].meta[\"filename_or_obj\"])[1]\n",
        "    print(\"Visualizing image transforms for: {} ...\\n\".format(img_name))\n",
        "\n",
        "    # Get original image and label info\n",
        "    img_path = data_root + '/val/images/' + img_name\n",
        "    # Workaround for _crop in image filenames not in labels\n",
        "    if '_crop' in img_name:\n",
        "        img_name = str.split(img_name, '_crop')[0]\n",
        "    idx = find_dict_idx_by_val(val_files, 'label', img_name)\n",
        "    label_path = val_files[idx]['label']\n",
        "    orig_img = nib.load(img_path)\n",
        "    orig_img_data = orig_img.get_fdata()\n",
        "    orig_label = nib.load(label_path)\n",
        "    orig_label_data = orig_label.get_fdata()\n",
        "\n",
        "    # Get transformed image and label info\n",
        "    img = val_ds[img_idx][\"image\"]\n",
        "    label = val_ds[img_idx][\"label\"]\n",
        "    val_inputs = torch.unsqueeze(img, 1).cuda()\n",
        "    print(\"Original image shape: \", val_inputs.shape)\n",
        "    val_labels = torch.unsqueeze(label, 1).cuda()\n",
        "    print(\"Transformed image shape: \", val_labels.shape)\n",
        "\n",
        "    # Plot images\n",
        "    plt.figure(\"Orig vs Transformed\", (22, 6))\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.title(\"original image\")\n",
        "    plt.imshow(orig_img_data[:, :, slice_num], cmap='gray')\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.title(\"original label\")\n",
        "    plt.imshow(orig_label_data[:, :, slice_num], cmap='gray')\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.title(\"transformed image\")\n",
        "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, slice_num], cmap=\"gray\")\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.title(\"transformed label\")\n",
        "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, slice_num])\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xXF75eO1_Nxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check a transformed image from the train dataset\n",
        "\n",
        "# Check an image from the train dataset\n",
        "demo_train_data = first(train_loader)\n",
        "image, label = demo_train_data[\"image\"][0][0], demo_train_data[\"label\"][0][0]\n",
        "print(\"Tranformed train image shape: {}, label shape: {}\".format(image.shape, label.shape))\n",
        "slice_num = 52 # @param {\"type\":\"slider\",\"min\":0,\"max\":96,\"step\":1}\n",
        "\n",
        "plt.figure(\"image\", (18, 6))\n",
        "plt.imshow(image[slice_num, :, :].detach().cpu(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e36AVjkFX2Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "2qNz4c-a16kn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3YicbXZPFOL"
      },
      "outputs": [],
      "source": [
        "#@title Set model parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "out_channels = 1 #@param\n",
        "img_size = 96 #@param\n",
        "feature_size = 70 # @param [\"16\",\"32\",\"48\",\"64\",\"70\",\"96\"] {\"type\":\"raw\"}\n",
        "hidden_size = 768 #@param\n",
        "mlp_dim = 3072 #@param\n",
        "num_heads = 12 #@param\n",
        "norm_name = \"instance\" # @param [\"instance\",\"batch\"]\n",
        "res_block = True # @param {\"type\":\"boolean\"}\n",
        "conv_block = True # @param {\"type\":\"boolean\"}\n",
        "dropout_rate = 0.0 #@param\n",
        "\n",
        "model = UNETR(\n",
        "    in_channels=1,\n",
        "    out_channels=out_channels,\n",
        "    img_size=(img_size, img_size, img_size),\n",
        "    feature_size=feature_size,\n",
        "    hidden_size=hidden_size,\n",
        "    mlp_dim=mlp_dim,\n",
        "    num_heads=num_heads,\n",
        "    proj_type=\"perceptron\",\n",
        "    norm_name=norm_name,\n",
        "    conv_block=conv_block,\n",
        "    res_block=res_block,\n",
        "    dropout_rate=dropout_rate,\n",
        ").to(device)\n",
        "\n",
        "if out_channels > 1:\n",
        "    loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
        "\n",
        "else:\n",
        "    loss_function = DiceCELoss(include_background=True, sigmoid=True)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True #@param\n",
        "lr = 1e-6 #@param\n",
        "weight_decay = 0 # @param [\"1e-2\",\"0\",\"1e-3\",\"1e-4\"] {\"type\":\"raw\"}\n",
        "amsgrad = True # @param {\"type\":\"boolean\"}\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "optimizer = Novograd(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CEgz2a6ePFOM"
      },
      "outputs": [],
      "source": [
        "#@title Train pipeline\n",
        "# Modified from : Copyright (c) MONAI Consortium and [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "# Save each new training attempt results by name (TRAIN_SES_NUM)\n",
        "saved_models_dir = \"saved_models\" # @param [\"saved_models\"] {\"allow-input\":true}\n",
        "saved_model_path, TRAIN_SESS_NUM = set_saved_model_path(out_dir, saved_models_dir)\n",
        "\n",
        "def validation(epoch_iterator_val):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in epoch_iterator_val:\n",
        "            val_inputs, val_labels = (batch[\"image\"].to(device), batch[\"label\"].to(device))\n",
        "            #print(\"\\nValidation input shape: \", val_inputs.shape)\n",
        "            #print(\"\\nValidation ground truth shape: \", val_labels.shape)\n",
        "            val_outputs = sliding_window_inference(val_inputs, roi_size=(img_size, img_size, img_size), sw_batch_size=4, predictor=model)\n",
        "            #print(\"\\nValidation output shape: \", val_outputs.shape)\n",
        "            val_labels_list = decollate_batch(val_labels)\n",
        "            val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
        "            val_outputs_list = decollate_batch(val_outputs)\n",
        "            val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
        "            #print(\"Val output convert: \", val_output_convert)\n",
        "            #print(\"Val labels convert: \", val_labels_convert)\n",
        "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
        "            epoch_iterator_val.set_description(\"Validate (%d / %d Steps)\" % (global_step, 10.0))  # noqa: B038\n",
        "        mean_dice_val = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "    return mean_dice_val\n",
        "\n",
        "\n",
        "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        step += 1\n",
        "        x, y = (batch[\"image\"].to(device), batch[\"label\"].to(device))\n",
        "        #print(\"\\nModel input shape: \", x.shape)\n",
        "        logit_map = model(x)\n",
        "        #print(\"\\nModel output shape: \", logit_map.shape)\n",
        "        #print(\"\\nGround truth shape: \", y.shape)\n",
        "        loss = loss_function(logit_map, y)\n",
        "        loss.backward()\n",
        "        epoch_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        epoch_iterator.set_description(  # noqa: B038\n",
        "            \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, loss)\n",
        "        )\n",
        "        if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
        "            epoch_iterator_val = tqdm(val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True)\n",
        "            dice_val = validation(epoch_iterator_val)\n",
        "            epoch_loss /= step\n",
        "            epoch_loss_values.append(epoch_loss)\n",
        "            metric_values.append(dice_val)\n",
        "            if dice_val > dice_val_best:\n",
        "                dice_val_best = dice_val\n",
        "                global_step_best = global_step\n",
        "                torch.save(model.state_dict(), saved_model_path)\n",
        "                print(\n",
        "                    \"\\033[32m Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\\033[0m\".format(dice_val_best, dice_val)\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    \"\\033[33mModel Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\\033[0m\".format(\n",
        "                        dice_val_best, dice_val\n",
        "                    )\n",
        "                )\n",
        "        global_step += 1\n",
        "    return global_step, dice_val_best, global_step_best\n",
        "\n",
        "max_iterations = 100 #@param\n",
        "eval_num = 10 #@param\n",
        "post_label = Compose([AsDiscrete(n_classes=out_channels)])\n",
        "post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
        "#post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold_values=True)])\n",
        "#post_trans = Compose([EnsureType(), Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "global_step = 0\n",
        "dice_val_best = -0.1\n",
        "global_step_best = 0\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "while global_step < max_iterations:\n",
        "    global_step, dice_val_best, global_step_best = train(global_step, train_loader, dice_val_best, global_step_best)\n",
        "model.load_state_dict(torch.load(os.path.join(saved_models_dir, saved_model_path)))\n",
        "\n",
        "print(f\"\\n\\n\\n train completed, best_metric: {dice_val_best:.4f} \" f\"at iteration: {global_step_best}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert notebook to txt file and save for track record of train parameters for each saved model\n",
        "\n",
        "# Define train notebooks dir\n",
        "train_nbs_path = \"train_notebooks\"\n",
        "train_nbs_dir = os.path.join(out_dir, train_nbs_path)\n",
        "if not os.path.exists(train_nbs_dir):\n",
        "    os.mkdir(train_nbs_dir)\n",
        "\n",
        "\n",
        "# Save train notebook to text file\n",
        "train_nb_path = train_nbs_dir + '/' + str(TRAIN_SESS_NUM)\n",
        "\n",
        "# Original train notebook (.ipynb) location\n",
        "orig_nb_dir = \"/content/drive/MyDrive/'Colab Notebooks'/\" # @param [\"/content/drive/MyDrive/'Colab Notebooks'/\"] {\"allow-input\":true}\n",
        "fn = \"Cacao_Whole_Flower_Seg_unetr.ipynb\" # @param [\"Whole_Flower_Seg_unetr_newpipeline_abdomendata.ipynb\"] {\"allow-input\":true}\n",
        "fpath = orig_nb_dir + fn\n",
        "\n",
        "# Convert train notebook to text file\n",
        "%cd $orig_nb_dir\n",
        "!jupyter nbconvert --output-dir=$train_nb_path --to python $fpath\n",
        "\n",
        "print(\"Colab notebook: {} converted to text file: {} for use as train parameters log\".format(fpath, train_nb_path))"
      ],
      "metadata": {
        "id": "4_urTaCYTOg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot train Loss and mean DICE and save results to file\n",
        "# Modified from : Copyright (c) MONAI Consortium and [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "# Plot figures\n",
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Iteration Average Loss\")\n",
        "x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
        "y = epoch_loss_values\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
        "y = metric_values\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.plot(x, y)\n",
        "plt.show()\n",
        "\n",
        "# Define train graphs dir\n",
        "train_graphs_dir = os.path.join(out_dir, \"train_graphs\")\n",
        "if not os.path.exists(train_graphs_dir):\n",
        "    os.mkdir(train_graphs_dir)\n",
        "\n",
        "# Save train graph to file\n",
        "train_graph_path = train_graphs_dir + TRAIN_SESS_NUM + '.png'\n",
        "plt.savefig(train_graph_path)"
      ],
      "metadata": {
        "id": "FAevzAg_JHpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot model prediction vs ground truth\n",
        "\n",
        "val_img_idx = 4 # @param {\"type\":\"slider\",\"min\":0,\"max\":5,\"step\":1}\n",
        "slice_num = 186 # @param {\"type\":\"slider\",\"min\":0,\"max\":500,\"step\":1}\n",
        "\n",
        "# Load in saved model\n",
        "saved_models_dir = os.path.join(out_dir, \"saved_models\")\n",
        "use_last_model = True # @param {\"type\":\"boolean\"}\n",
        "if use_last_model:\n",
        "    last_attempt = !ls $saved_models_dir | tail -n 1\n",
        "    TRAIN_SESS_NUM = int(os.path.splitext(last_attempt[0])[0])\n",
        "    if TRAIN_SESS_NUM < 10:\n",
        "        TRAIN_SESS_NUM = '0' + str(TRAIN_SESS_NUM)\n",
        "    else:\n",
        "        TRAIN_SESS_NUM = str(TRAIN_SESS_NUM)\n",
        "else:\n",
        "    TRAIN_SESS_NUM = \"02\" # @param [\"01\",\"02\",\"03\"] {\"allow-input\":true}\n",
        "\n",
        "saved_model_path = saved_models_dir + '/' + TRAIN_SESS_NUM + \".pth\"\n",
        "model.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# Show model predictions on image\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Check an image from the val dataset\n",
        "    img_name = os.path.split(val_ds[val_img_idx]['image'].meta[\"filename_or_obj\"])[1]\n",
        "    img = val_ds[val_img_idx][\"image\"]\n",
        "    label = val_ds[val_img_idx][\"label\"]\n",
        "    val_inputs = torch.unsqueeze(img, 1).cuda()\n",
        "    print(val_inputs.shape)\n",
        "    val_labels = torch.unsqueeze(label, 1).cuda()\n",
        "    print(val_labels.shape)\n",
        "    val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model, overlap=0.8)\n",
        "    print(val_outputs.shape)\n",
        "    plt.figure(\"check\", (18, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"image\")\n",
        "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, slice_num], cmap=\"gray\")\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"label\")\n",
        "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, slice_num])\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"output\")\n",
        "    if out_channels > 1:\n",
        "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, 0, :, :, slice_num])\n",
        "    else:\n",
        "        plt.imshow((val_outputs > 0.5).int().detach().cpu()[0, 0, :, :, slice_num])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0VDCsgOuWA6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}