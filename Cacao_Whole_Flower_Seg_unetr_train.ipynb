{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/CV_for_flower_CT/blob/main/Cacao_Whole_Flower_Seg_unetr_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iHpIGLEPFOB"
      },
      "source": [
        "# Using MONAI Label for shape prediction of flowers from micro-CT images\n",
        "---   \n",
        "*Last Updated 31 Mar 2023*  \n",
        "-Runs in Python 3 with Monai 1.5-   \n",
        "This notebook uses a dataset of micro-CT images and segementations (both as nifti files, .nii.gz) to custom train a UNETR Computer Vision Model ([Hatamizadeh et al. 2021](https://arxiv.org/abs/2103.10504)) using the [MONAI Label](https://github.com/aubricot/computer_vision_with_eol_images/tree/master/classification_for_image_tagging) framework. U-Nets are a type of Convolutional Neural Network (CNN) shaped like a \"U\" with pairings of encoders (contracting path) and decoders (expanding path) who have gained popularity in recent years as a tool for segmentation of 3D image datasets ([Tomar 2021](https://medium.com/analytics-vidhya/what-is-unet-157314c87634&ved=2ahUKEwjeiK-myoyMAxX-QzABHb85DmQQFnoECBEQAQ&usg=AOvVaw1V_3fmv2Fckxy0QTuwS6LM)). UNETR has a similar network design/shape for the encoder and decoder as a U-Net, but UNET TRansformers (UNETR), uses a transformer as the encoder.\n",
        "\n",
        "_References_\n",
        "\n",
        "* [Hatamizadeh et al. 2021](https://arxiv.org/abs/2103.10504)   \n",
        "* [Tomar 2021](https://medium.com/analytics-vidhya/what-is-unet-157314c87634&ved=2ahUKEwjeiK-myoyMAxX-QzABHb85DmQQFnoECBEQAQ&usg=AOvVaw1V_3fmv2Fckxy0QTuwS6LM)\n",
        "\n",
        "_License_   \n",
        "This notebook is licensed under an [MIT license](https://github.com/aubricot/CV_for_flower_CT/blob/master/LICENSE). Portions are modified from the MONAI Label [3D Multi-organ Segmentation with UNETR (BTCV Challenge)](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/unetr_btcv_segmentation_3d.ipynb) following licensing terms under Copyright (c) MONAI Consortium and [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).\n",
        "\n",
        "The sections \"Make sure runtime is set to GPU and High RAM,\" \"Train pipeline,\" and \"Plot train Loss and mean DICE and save results to file\" were modified from 3D Multi-organ Segmentation with UNETR (BTCV Challenge) to better suit our needs. Google Colab form fields were also added, as well as tools to split the dataset train-validate-test, methods to save training outputs (trained models, notebook in its state for each train run as a txt file, and train graphs; named based on the train attempt number (ie the first train attempt files were saved as 01.ph, 01.txt, and 01.png)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRH49cpJzPdk"
      },
      "source": [
        "to check\n",
        "https://github.com/Project-MONAI/MONAI/discussions/4023\n",
        "https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d.ipynb\n",
        "https://github.com/Project-MONAI/MONAI/discussions/7267\n",
        "https://docs.monai.io/en/stable/optimizers.html\n",
        "https://github.com/Project-MONAI/tutorials/blob/main/acceleration/fast_training_tutorial.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG6BekRX0YUN"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxiZicF74BXj"
      },
      "outputs": [],
      "source": [
        "#@title Make sure runtime is set to GPU and High RAM\n",
        "# Copyright (c) MONAI Consortium and [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z45TOUuL2we9"
      },
      "outputs": [],
      "source": [
        "#@title Check how many workers are suggested for your system\n",
        "# TO DO: Adjust num_workers throughout code to speed or slow performance based on available (cloud) hardware\n",
        "import os\n",
        "\n",
        "cpu_count = os.cpu_count()\n",
        "if cpu_count is not None:\n",
        "    max_num_worker_suggest = cpu_count\n",
        "\n",
        "print(\"Suggested max number of workers is: \", max_num_worker_suggest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzKDsTXQfAmj"
      },
      "outputs": [],
      "source": [
        "#@title Choose where to save results\n",
        "\n",
        "# Use dropdown menu on right\n",
        "save = \"in my Google Drive\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "\n",
        "# Mount google drive to export image tagging file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Type in the path to your project wd in form field on right\n",
        "wd = \"/content/drive/MyDrive/Miami_21/project/manuscripts/CV_for_flower_CT/analysis/data\" # @param [\"/content/drive/MyDrive/Miami_21/project/manuscripts/CV_for_flower_CT/analysis/data\",\"/content/drive/MyDrive/Miami_21/project/manuscripts/CV_for_flower_CT/analysis/demo_data/Abdomen/Abdomen/RawData/data\"] {\"allow-input\":true}\n",
        "print(\"Working with data from: \\n\")\n",
        "%cd $wd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMR-JPJ_-aGn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Unzip dataset converted and zipped with preprocessing_nrrd2nifti.ipynb\n",
        "# Enter wd, zip and unzipped filenames using form field to right\n",
        "wd = \"/content/drive/MyDrive/Miami_21/project/manuscripts/CV_for_flower_CT/analysis/data/input\" #@param [\"/content/drive/MyDrive\"] {allow-input: true}\n",
        "zip_fn = \"nii_gzs5_masked.zip\" #@param [\"PlantCLEF2015TrainingData_2.zip\"] {allow-input: true}\n",
        "zip_fpath = wd + '/' + zip_fn\n",
        "unzipped_fn = \"nii_gzs5_masked\" #@param [\"images\"] {allow-input: true}\n",
        "unzipped_fpath = wd + '/' + unzipped_fn\n",
        "\n",
        "# Unzip the file/folder\n",
        "!unzip $zip_fpath -d $unzipped_fpath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDXTgDy0PFOE"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "!pip install -q \"monai-weekly[nibabel, tqdm, einops]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!pip install nbconvert\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-myucD_YPFOG"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import glob\n",
        "import math\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from monai.utils import first\n",
        "from monai.losses import DiceCELoss, DiceLoss\n",
        "from monai.inferers import sliding_window_inference, SimpleInferer\n",
        "from monai.transforms import (\n",
        "    Activations,\n",
        "    AsDiscrete,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    EnsureChannelFirstd,\n",
        "    EnsureType,\n",
        "    EnsureTyped,\n",
        "    FgBgToIndicesd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandFlipd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    RandShiftIntensityd,\n",
        "    RandSpatialCropd,\n",
        "    Resized,\n",
        "    ScaleIntensityRangePercentilesd,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    RandRotate90d,\n",
        ")\n",
        "\n",
        "from monai.config import print_config\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.networks.layers import Act, Norm\n",
        "from monai.networks.nets import UNETR, unet\n",
        "from torch.optim import Adam, SGD\n",
        "from monai.optimizers import Novograd\n",
        "from monai.apps import download_and_extract\n",
        "from monai.data import NibabelReader\n",
        "\n",
        "from monai.data import (\n",
        "    DataLoader,\n",
        "    CacheDataset,\n",
        "    load_decathlon_datalist,\n",
        "    decollate_batch,\n",
        "    ThreadDataLoader,\n",
        "    Dataset,\n",
        "    set_track_meta,\n",
        ")\n",
        "\n",
        "# Facilitate debugging by making GPU tell more useful information\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "\n",
        "# Check that cuda is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available and being used.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available, using CPU instead.\")\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOMgBexnmrIw"
      },
      "outputs": [],
      "source": [
        "#@title Set up data directory\n",
        "os.environ['MONAI_DATA_DIRECTORY'] = wd\n",
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = directory\n",
        "print(f\"root dir is: {root_dir}\")\n",
        "\n",
        "out_dir = wd + '/' + \"results/\"\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "print(f\"out dir is: {out_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I96sR9xO3TZU"
      },
      "outputs": [],
      "source": [
        "#@title Define functions\n",
        "\n",
        "# Save each new training attempt results by name (TRAIN_SES_NUM)\n",
        "def set_saved_model_path(out_dir = out_dir, saved_models_dir = \"saved_models\"):\n",
        "    saved_models_wd = os.path.join(out_dir, saved_models_dir)\n",
        "    if not os.path.exists(saved_models_wd):\n",
        "        print(\"{} not found. Making saved models directory at: {}\".format(saved_models_dir, saved_models_wd))\n",
        "        os.mkdir(saved_models_wd)\n",
        "    # Name folder to sort by attempt number (useful if many training runs)\n",
        "    else:\n",
        "        try:\n",
        "            last_attempt = !ls $saved_models_wd | tail -n 1\n",
        "            last_attempt = int(os.path.splitext(last_attempt[0])[0])\n",
        "        except:\n",
        "            last_attempt = 0\n",
        "        if last_attempt < 9:\n",
        "            TRAIN_SESS_NUM = \"0\" + str(last_attempt + 1)\n",
        "        else:\n",
        "            TRAIN_SESS_NUM = str(last_attempt + 1)\n",
        "\n",
        "    saved_model_path = os.path.join(saved_models_wd, (TRAIN_SESS_NUM + \".pth\"))\n",
        "    print(\"Saving trained model results to: \", saved_model_path)\n",
        "\n",
        "    return saved_model_path, TRAIN_SESS_NUM\n",
        "\n",
        "# For pulling img and label file names from train/val dictionaries\n",
        "def find_dict_idx_by_val(dict_list, key, val):\n",
        "    for idx, dict in enumerate(dict_list):\n",
        "        if key in dict and val in dict[key]:\n",
        "            return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-test-validation split"
      ],
      "metadata": {
        "id": "U0rvGFcLEmML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GH8jTIcizTyR"
      },
      "outputs": [],
      "source": [
        "#@title Train-test-validation split\n",
        "!pip install split-folders\n",
        "import splitfolders\n",
        "\n",
        "# List all files in dir before splitting\n",
        "all_vols = os.listdir('./images/class1')\n",
        "n = len(all_vols)\n",
        "print(\"Training dataset size of N = \", n)\n",
        "\n",
        "# Save list of files before split to txt file in your wd\n",
        "with open(\"all_vols.txt\", \"w\") as output:\n",
        "    output.write(str(all_vols))\n",
        "print(\"List of all files before splitting saved to: \", (wd + '/all_vols.txt'))\n",
        "\n",
        "# Train-test-validation split (70% - 15% - 15%)\n",
        "splitfolders.ratio(\"images\", output=\"input\", seed=n,\n",
        "                   ratio=(0.7, 0.15, 0.15), group_prefix=None, move=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Pqu-8yuhAhMw"
      },
      "outputs": [],
      "source": [
        "#@title Move labels to their corresponding train-test-val folders to match images\n",
        "\n",
        "# Get list of all labels\n",
        "all_labs = os.listdir('./labels/class1')\n",
        "all_labs = [os.path.splitext(os.path.splitext(file)[0])[0] for file in all_labs] # Remove .nii.gz extensions\n",
        "all_labs = [file.removesuffix('_seg') for file in all_labs]\n",
        "\n",
        "# Find which label files need to be moved to train-test-val and move them\n",
        "train = os.listdir('./input/train/class1')\n",
        "train = [os.path.splitext(os.path.splitext(file)[0])[0] for file in train] # Remove .nii.gz extensions\n",
        "train = [file.removesuffix('_crop') for file in train]\n",
        "train = [file.removesuffix('_cropped') for file in train]\n",
        "train = [file.removesuffix('_crop_mask') for file in train]\n",
        "\n",
        "test = os.listdir('./input/test/class1')\n",
        "test = [os.path.splitext(os.path.splitext(file)[0])[0] for file in test] # Remove .nii.gz extensions\n",
        "test = [file.removesuffix('_crop') for file in test]\n",
        "test = [file.removesuffix('_cropped') for file in test]\n",
        "test = [file.removesuffix('_crop_mask') for file in test]\n",
        "\n",
        "val = os.listdir('./input/val/class1')\n",
        "val = [os.path.splitext(os.path.splitext(file)[0])[0] for file in val] # Remove .nii.gz extensions\n",
        "val = [file.removesuffix('_crop') for file in val]\n",
        "val = [file.removesuffix('_cropped') for file in val]\n",
        "val = [file.removesuffix('_crop_mask') for file in val]\n",
        "\n",
        "train_mv = set(train) & set(all_labs)\n",
        "test_mv = set(test) & set(all_labs)\n",
        "val_mv = set(val) & set(all_labs)\n",
        "\n",
        "# Move files to train\n",
        "os.makedirs('input/train/class1/labels')\n",
        "for file in train_mv:\n",
        "    file_path = os.path.join('labels/class1', file + '_seg.nii.gz')\n",
        "    target = os.path.join('input/train/class1/labels', file + '_seg.nii.gz')\n",
        "    shutil.copy2(file_path, target)  # Preserves metadata\n",
        "    print(f\"Copied: {file_path} to {target}\")\n",
        "\n",
        "# Move files to test\n",
        "os.makedirs('input/test/class1/labels')\n",
        "for file in test_mv:\n",
        "    file_path = os.path.join('labels/class1', file + '_seg.nii.gz')\n",
        "    target = os.path.join('input/test/class1/labels', file + '_seg.nii.gz')\n",
        "    shutil.copy2(file_path, target)  # Preserves metadata\n",
        "    print(f\"Copied: {file_path} to {target}\")\n",
        "\n",
        "# Move files to validation (val)\n",
        "os.makedirs('input/val/class1/labels')\n",
        "for file in val_mv:\n",
        "    file_path = os.path.join('labels/class1', file + '_seg.nii.gz')\n",
        "    target = os.path.join('input/val/class1/labels', file + '_seg.nii.gz')\n",
        "    shutil.copy2(file_path, target)  # Preserves metadata\n",
        "    print(f\"Copied: {file_path} to {target}\")\n",
        "\n",
        "# Move all image files within train-test-val into images/\n",
        "target = 'input/train/class1/images'\n",
        "os.makedirs(target)\n",
        "source = './input/train/class1'\n",
        "files = os.listdir(source)\n",
        "for file in files:\n",
        "    shutil.move(os.path.join(source, file), target)\n",
        "    print(f\"Moved: {source} to {target}\")\n",
        "\n",
        "target = 'input/test/class1/images'\n",
        "os.makedirs(target)\n",
        "source = './input/test/class1'\n",
        "files = os.listdir(source)\n",
        "for file in files:\n",
        "    shutil.move(os.path.join(source, file), target)\n",
        "    print(f\"Moved: {source} to {target}\")\n",
        "\n",
        "target = 'input/val/class1/images'\n",
        "os.makedirs(target)\n",
        "source = './input/val/class1'\n",
        "files = os.listdir(source)\n",
        "for file in files:\n",
        "    shutil.move(os.path.join(source, file), target)\n",
        "    print(f\"Moved: {source} to {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z35Y-MlcPjcJ"
      },
      "source": [
        "## Transform datasets for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzKIGvLJrjV7"
      },
      "outputs": [],
      "source": [
        "#@title Get dataset filepaths for train and validation\n",
        "data_root = os.path.join(root_dir, \"input\")\n",
        "\n",
        "train_images = sorted(glob.glob(os.path.join(data_root, \"train/images\", \"*.nii.gz\")))\n",
        "train_labels = sorted(glob.glob(os.path.join(data_root, \"train/labels\", \"*.nii.gz\")))\n",
        "val_images = sorted(glob.glob(os.path.join(data_root, \"val/images\", \"*.nii.gz\")))\n",
        "val_labels = sorted(glob.glob(os.path.join(data_root, \"val/labels\", \"*.nii.gz\")))\n",
        "\n",
        "train_files = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
        "val_files = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(val_images, val_labels)]\n",
        "num_train = len(train_files)\n",
        "num_val = len(val_files)\n",
        "\n",
        "print(\"Training model with {} train images and {} validation images\\n\".format(num_train, num_val))\n",
        "print(\" Train images loaded from {} \\n Train labels loaded from {} \\n Validation images loaded from {} \\n Validation labels loaded from {}\".format(train_images, train_labels, val_images, val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XTwTQrKHIJO"
      },
      "outputs": [],
      "source": [
        "#@title Dataset Transforms\n",
        "\n",
        "# use clip with ScaleIntensityRangePercentilesd?\n",
        "clip = False # @param {\"type\":\"boolean\"}\n",
        "relative = False # @param {\"type\":\"boolean\"}\n",
        "\n",
        "# Weights of pos vs neg labels for RandCropByPosNegLabeld\n",
        "pos = 1 #@param\n",
        "neg = 1 #@param\n",
        "\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(.05, .05, .05),\n",
        "            mode=(\"bilinear\", \"nearest\"),),\n",
        "        ScaleIntensityRangePercentilesd(\n",
        "                keys=\"image\",\n",
        "                lower=10, upper=90, b_min=0, b_max=200,\n",
        "                clip=clip,\n",
        "                relative=relative,),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        RandCropByPosNegLabeld(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            label_key=\"label\",\n",
        "            spatial_size=(96, 96, 96),\n",
        "            pos=pos, # 1 to 1 ratio pos:neg applies equal weight; since binary problem, apply more weight to pos\n",
        "            neg=neg,\n",
        "            num_samples=4,\n",
        "            image_key=\"image\",\n",
        "            image_threshold=0,),\n",
        "    ]\n",
        ")\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(.05, .05, .05),\n",
        "            mode=(\"bilinear\", \"nearest\"),),\n",
        "        ScaleIntensityRangePercentilesd(\n",
        "                keys=\"image\",\n",
        "                lower=10, upper=90, b_min=0, b_max=200,\n",
        "                clip=clip,\n",
        "                relative=relative,),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yESVL-6zIOLm"
      },
      "outputs": [],
      "source": [
        "#@title Transform datasets and read into cache for training\n",
        "\n",
        "# Train Dataset\n",
        "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_num=num_train, cache_rate=1.0, num_workers=8)\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=6, pin_memory=True)\n",
        "\n",
        "# Validation Dataset\n",
        "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_num=num_val, cache_rate=1.0, num_workers=8)\n",
        "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "print(\"\\n\\nSample Validation image shape after transform: {}, label shape: {}\".format(first(val_loader)['image'].shape, first(val_loader)['label'].shape))\n",
        "print(\"\\n\\nSample Train image shape after transform: {}, label shape: {}\".format(first(train_loader)['image'].shape, first(train_loader)['label'].shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKWK0qdE5U2T"
      },
      "outputs": [],
      "source": [
        "#@title Inspect image and labels after transformation\n",
        "import nibabel as nib\n",
        "\n",
        "# Coordinate system (for Nifti it's RAS)\n",
        "axcodes = \"RAS\" # @param [\"RAS\",\"PLI\",\"LPS\"] {\"allow-input\":true}\n",
        "\n",
        "# Check transformed image shape and loading\n",
        "load_transform = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(.05, .05, .05),\n",
        "            mode=(\"bilinear\")),\n",
        "        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7tdpDzH5W07"
      },
      "outputs": [],
      "source": [
        "#@title Set val image index and slice number to visualize\n",
        "\n",
        "img_idx = 2 # @param {\"type\":\"slider\",\"min\":0,\"max\":2,\"step\":1}\n",
        "slice_num = 148 # @param {\"type\":\"slider\",\"min\":0,\"max\":500,\"step\":1}\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Check an image from the val dataset\n",
        "    img_name = os.path.split(val_ds[img_idx]['image'].meta[\"filename_or_obj\"])[1]\n",
        "    print(\"Visualizing image transforms for: {} ...\\n\".format(img_name))\n",
        "\n",
        "    # Get original image and label info\n",
        "    img_path = data_root + '/val/images/' + img_name\n",
        "    # Workaround for _crop in image filenames not in labels\n",
        "    if '_crop' in img_name:\n",
        "        img_name = str.split(img_name, '_crop')[0]\n",
        "    idx = find_dict_idx_by_val(val_files, 'image', img_name)\n",
        "    orig_data = load_transform(val_files[idx])\n",
        "    orig_img = orig_data[\"image\"]\n",
        "    print(\"Original image shape: \", orig_img.shape)\n",
        "    orig_label = orig_data[\"label\"]\n",
        "    print(\"Original label shape: \", orig_label.shape)\n",
        "\n",
        "    # Get transformed image and label info\n",
        "    img = val_ds[img_idx][\"image\"]\n",
        "    label = val_ds[img_idx][\"label\"]\n",
        "    print(\"Transformed image shape: \", img.shape)\n",
        "    print(\"Transformed label shape: \", label.shape)\n",
        "\n",
        "    # Plot images\n",
        "    plt.figure(\"Orig vs Transformed\", (22, 6))\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.title(\"original image\")\n",
        "    plt.imshow(orig_img.numpy()[0, :, :, slice_num], cmap='gray')\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.title(\"original label\")\n",
        "    plt.imshow(orig_label.numpy()[0, :, :, slice_num], cmap='gray')\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.title(\"transformed image\")\n",
        "    plt.imshow(img.cpu().numpy()[0, :, :, slice_num], cmap=\"gray\")\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.title(\"transformed label\")\n",
        "    plt.imshow(label.cpu().numpy()[0, :, :, slice_num])\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qNz4c-a16kn"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3YicbXZPFOL"
      },
      "outputs": [],
      "source": [
        "#@title Set model parameters\n",
        "\n",
        "# Use cuda GPU if available, else use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# How many epochs to train for and how often to evaluate model performance\n",
        "max_iterations = 2400 #@param\n",
        "eval_num = 20 #@param\n",
        "\n",
        "# Parameters\n",
        "out_channels = 1 #@param\n",
        "img_size = 96 #@param\n",
        "feature_size = 32 # @param [\"16\",\"32\",\"48\",\"64\",\"70\",\"96\"] {\"type\":\"raw\"}\n",
        "hidden_size = 768 #@param\n",
        "mlp_dim = 3072 #@param\n",
        "num_heads = 12 #@param\n",
        "norm_name = \"instance\" # @param [\"instance\",\"batch\"]\n",
        "res_block = True # @param {\"type\":\"boolean\"}\n",
        "conv_block = True # @param {\"type\":\"boolean\"}\n",
        "dropout_rate = 0.0 #@param\n",
        "pos_embed = \"perceptron\" # @param [\"conv\",\"perceptron\"]\n",
        "\n",
        "# Define the model\n",
        "model = UNETR(\n",
        "    in_channels=1,\n",
        "    out_channels=out_channels,\n",
        "    img_size=(img_size, img_size, img_size),\n",
        "    feature_size=feature_size,\n",
        "    hidden_size=hidden_size,\n",
        "    mlp_dim=mlp_dim,\n",
        "    num_heads=num_heads,\n",
        "    norm_name=norm_name,\n",
        "    conv_block=conv_block,\n",
        "    res_block=res_block,\n",
        "    dropout_rate=dropout_rate,\n",
        "    proj_type=pos_embed,\n",
        ").to(device)\n",
        "\n",
        "# Get model architecture info for pubs, etc\n",
        "#print(len(model.model))\n",
        "#print(model)\n",
        "\n",
        "# Define the loss function\n",
        "if out_channels > 1:\n",
        "    loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
        "else:\n",
        "    loss_function = DiceCELoss(include_background=False, sigmoid=True)\n",
        "\n",
        "# Algorithmically optimize GPU training and speed things up, could be variable for val images since size varies\n",
        "torch.backends.cudnn.benchmark = True #@param\n",
        "\n",
        "# Set optimizer parameters\n",
        "optimizer = \"AdamW\" # @param [\"AdamW\",\"Novograd\"] {\"allow-input\":true}\n",
        "lr = 0.8e-6 # @param [\"1.5e-3\",\"1e-3\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "weight_decay = 2e-3 # @param [\"1e-2\",\"0\",\"1e-3\",\"1e-4\",\"1e-1\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "amsgrad = False # @param {\"type\":\"boolean\"}\n",
        "if \"Adam\" in optimizer:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "else:\n",
        "    optimizer = Novograd(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY0uFV-RSFHH"
      },
      "outputs": [],
      "source": [
        "#@title Find optimal learning rate based on model parameters\n",
        "from monai.optimizers import LearningRateFinder\n",
        "\n",
        "# Define lower and upper learning rates to inspect\n",
        "lower_lr, upper_lr = 1e-6, 1\n",
        "# Add lower_lr to optimizer\n",
        "if \"Adam\" in optimizer:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lower_lr, weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "else:\n",
        "    optimizer = Novograd(model.parameters(), lr=lower_lr, weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "\n",
        "# Find optimal learning rate\n",
        "criterion = loss_function\n",
        "optimizer = optimizer\n",
        "lr_finder = LearningRateFinder(model, optimizer, criterion)\n",
        "lr_finder.range_test(train_loader, end_lr=upper_lr, num_iter=20)\n",
        "lr_finder.get_steepest_gradient()\n",
        "lr_finder.plot(log_lr=False)\n",
        "\n",
        "print(\"Steepest gradient found at: \", lr_finder.get_steepest_gradient())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "CEgz2a6ePFOM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#@title Train pipeline\n",
        "# Modified from : Copyright (c) MONAI Consortium and [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "# Save each new training attempt results by name (TRAIN_SES_NUM)\n",
        "saved_models_dir = \"saved_models\" # @param [\"saved_models\"] {\"allow-input\":true}\n",
        "saved_model_path, TRAIN_SESS_NUM = set_saved_model_path(out_dir, saved_models_dir)\n",
        "\n",
        "def validation(epoch_iterator_val):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in epoch_iterator_val:\n",
        "            val_inputs, val_labels = (batch[\"image\"].to(device), batch[\"label\"].to(device))\n",
        "            #print(\"\\nValidation input shape: \", val_inputs.shape)\n",
        "            #print(\"\\nValidation ground truth shape: \", val_labels.shape)\n",
        "            val_outputs = sliding_window_inference(val_inputs, roi_size=(img_size, img_size, img_size), sw_batch_size=4, predictor=model)\n",
        "            #print(\"\\nValidation output shape: \", val_outputs.shape)\n",
        "            val_labels_list = decollate_batch(val_labels)\n",
        "            val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
        "            val_outputs_list = decollate_batch(val_outputs)\n",
        "            val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
        "            #print(\"Val output convert: \", val_output_convert)\n",
        "            #print(\"Val labels convert: \", val_labels_convert)\n",
        "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
        "            epoch_iterator_val.set_description(\"Validate (%d / %d Steps)\" % (global_step, 10.0))  # noqa: B038\n",
        "        mean_dice_val = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "    return mean_dice_val\n",
        "\n",
        "\n",
        "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        step += 1\n",
        "        x, y = (batch[\"image\"].to(device), batch[\"label\"].to(device))\n",
        "        #print(\"\\nModel input shape: \", x.shape)\n",
        "        logit_map = model(x)\n",
        "        #print(\"\\nModel output shape: \", logit_map.shape)\n",
        "        #print(\"\\nGround truth shape: \", y.shape)\n",
        "        loss = loss_function(logit_map, y)\n",
        "        loss.backward()\n",
        "        epoch_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        epoch_iterator.set_description(  # noqa: B038\n",
        "            \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, loss)\n",
        "        )\n",
        "        if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
        "            epoch_iterator_val = tqdm(val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True)\n",
        "            dice_val = validation(epoch_iterator_val)\n",
        "            epoch_loss /= step\n",
        "            epoch_loss_values.append(epoch_loss)\n",
        "            metric_values.append(dice_val)\n",
        "            if dice_val > dice_val_best:\n",
        "                dice_val_best = dice_val\n",
        "                global_step_best = global_step\n",
        "                torch.save(model.state_dict(), saved_model_path)\n",
        "                print(\n",
        "                    \"\\033[32m Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\\033[0m\".format(dice_val_best, dice_val)\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    \"\\033[33mModel Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\\033[0m\".format(\n",
        "                        dice_val_best, dice_val\n",
        "                    )\n",
        "                )\n",
        "        global_step += 1\n",
        "    return global_step, dice_val_best, global_step_best\n",
        "\n",
        "post_label = Compose([AsDiscrete(n_classes=out_channels)])\n",
        "post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "global_step = 0\n",
        "dice_val_best = -0.1\n",
        "global_step_best = 0\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "while global_step < max_iterations:\n",
        "    global_step, dice_val_best, global_step_best = train(global_step, train_loader, dice_val_best, global_step_best)\n",
        "model.load_state_dict(torch.load(os.path.join(saved_models_dir, saved_model_path)))\n",
        "\n",
        "print(f\"\\n\\n\\n train completed, best_metric: {dice_val_best:.4f} \" f\"at iteration: {global_step_best}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_urTaCYTOg-"
      },
      "outputs": [],
      "source": [
        "#@title Convert notebook to txt file and save for track record of train parameters for each saved model\n",
        "\n",
        "# Define train notebooks dir\n",
        "train_nbs_path = \"train_notebooks\"\n",
        "train_nbs_dir = os.path.join(out_dir, train_nbs_path)\n",
        "if not os.path.exists(train_nbs_dir):\n",
        "    os.mkdir(train_nbs_dir)\n",
        "\n",
        "\n",
        "# Save train notebook to text file\n",
        "train_nb_path = train_nbs_dir + '/' + str(TRAIN_SESS_NUM)\n",
        "\n",
        "# Original train notebook (.ipynb) location\n",
        "orig_nb_dir = \"/content/drive/MyDrive/'Colab Notebooks'/\" # @param [\"/content/drive/MyDrive/'Colab Notebooks'/\"] {\"allow-input\":true}\n",
        "fn = \"Cacao_Whole_Flower_Seg_unetr.ipynb\" # @param [\"Whole_Flower_Seg_unetr_newpipeline_abdomendata.ipynb\"] {\"allow-input\":true}\n",
        "fpath = orig_nb_dir + fn\n",
        "\n",
        "# Convert train notebook to text file\n",
        "%cd $orig_nb_dir\n",
        "!jupyter nbconvert --output-dir=$train_nb_path --to python $fpath\n",
        "\n",
        "print(\"Colab notebook: {} converted to text file: {} for use as train parameters log\".format(fpath, train_nb_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAevzAg_JHpS"
      },
      "outputs": [],
      "source": [
        "#@title Plot train Loss and mean DICE and save results to file\n",
        "# Modified from : Copyright (c) MONAI Consortium and [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "# Plot figures\n",
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Iteration Average Loss\")\n",
        "x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
        "y = epoch_loss_values\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
        "y = metric_values\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.plot(x, y)\n",
        "\n",
        "# Define train graphs dir\n",
        "train_graphs_dir = os.path.join(out_dir, \"train_graphs/\")\n",
        "if not os.path.exists(train_graphs_dir):\n",
        "    os.mkdir(train_graphs_dir)\n",
        "\n",
        "# Save train graph to file\n",
        "train_graph_path = train_graphs_dir + TRAIN_SESS_NUM + '.png'\n",
        "plt.savefig(train_graph_path)\n",
        "print(\"Train graphs saved to: \", train_graph_path)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zYFwIyoCkIz"
      },
      "source": [
        "## Inspect Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VDCsgOuWA6a"
      },
      "outputs": [],
      "source": [
        "#@title Plot model prediction vs ground truth\n",
        "\n",
        "val_img_idx = 0 # @param {\"type\":\"slider\",\"min\":0,\"max\":2,\"step\":1}\n",
        "slice_num = 147 # @param {\"type\":\"slider\",\"min\":0,\"max\":500,\"step\":1}\n",
        "\n",
        "# Load in saved model\n",
        "saved_models_dir = os.path.join(out_dir, \"saved_models\")\n",
        "use_last_model = True # @param {\"type\":\"boolean\"}\n",
        "if use_last_model:\n",
        "    last_attempt = !ls $saved_models_dir | tail -n 1\n",
        "    TRAIN_SESS_NUM = int(os.path.splitext(last_attempt[0])[0])\n",
        "    if TRAIN_SESS_NUM < 10:\n",
        "        TRAIN_SESS_NUM = '0' + str(TRAIN_SESS_NUM)\n",
        "    else:\n",
        "        TRAIN_SESS_NUM = str(TRAIN_SESS_NUM)\n",
        "else:\n",
        "    TRAIN_SESS_NUM = \"38\" # @param [\"01\",\"02\",\"03\"] {\"allow-input\":true}\n",
        "\n",
        "saved_model_path = saved_models_dir + '/' + TRAIN_SESS_NUM + \".pth\"\n",
        "model.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "# Show model predictions on image\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Check an image from the val dataset\n",
        "    img_name = os.path.split(val_ds[val_img_idx]['image'].meta[\"filename_or_obj\"])[1]\n",
        "    img = val_ds[val_img_idx][\"image\"]\n",
        "    label = val_ds[val_img_idx][\"label\"]\n",
        "    val_inputs = torch.unsqueeze(img, 1).cuda()\n",
        "    print(val_inputs.shape)\n",
        "    val_labels = torch.unsqueeze(label, 1).cuda()\n",
        "    print(val_labels.shape)\n",
        "    val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model, overlap=0.8)\n",
        "    print(val_outputs.shape)\n",
        "    plt.figure(\"check\", (18, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"image\")\n",
        "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, slice_num], cmap=\"gray\")\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"label\")\n",
        "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, slice_num])\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"output\")\n",
        "    if out_channels > 1:\n",
        "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, 0, :, :, slice_num])\n",
        "    else:\n",
        "        plt.imshow((val_outputs > 0.5).int().detach().cpu()[0, 0, :, :, slice_num])\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}